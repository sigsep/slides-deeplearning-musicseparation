<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Deep Learning for Music Unmixing</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/lirmm.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/github.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<section class="cover" data-background="https://static.wixstatic.com/media/dc75cb_c43d95dcd4c84159a814f52e85bb98e0~mv2.png/v1/fill/w_2215,h_1263,al_c,q_95/dc75cb_c43d95dcd4c84159a814f52e85bb98e0~mv2.webp" data-background-repeat="no-repeat" data-background-position="bottom center" data-state="no-title-footer no-progressbar has-dark-background">
					<div class="covertitle">
					<h2 id="covertitle">Music Source Separation</h2>

					<p id="coverauthors">
						Fabian-Robert St√∂ter<br />
						<p>
							‚úâÔ∏è fabian@audioshake.ai<br />
						</p>
				
					</p>
					<p>
				</section>
			
				<section>
					<h1>Agenda</h1>
				
					<ul>
						<li>Introduction into Music Separation</li> 

						<li>Building blocks</li>
						<li>Architectures and Training</li>
						<li style="opacity: 0.4;">Discussion</li>
						<li>U-Net</li>
						<li>Trends</li>
						<li style="opacity: 0.4;">Discussion</li>
					</ul>
				</section>

				<section>
					<h1>About myself</h1>
							<ul>
								<li>PhD at AudioLabs with Bernd Edler</li>
								<li>Post-Doc at Inria in Montpellier, France</li>
								<li>Head-of-research at Audioshake</li>
							</ul>

							<p>
								<img width="100px" style="margin-right: 2em" src="assets/frs.jpg" alt=""> 
								<img width="200px" style="margin-right: 2em"  src="assets/AudioLabs.png" alt=""> 
								<img width="200px" style="margin-right: 2em; padding-top: 2em;" src="assets/inria.png" alt=""> 
								<img width="200px" style="background-color: rgb(54, 229, 183)" src="assets/AUDIOSHAKE_logo_white.webp" alt="">

							</p>
				</section>

				<section data-background-image="assets/stones_all.jpg"><h1>Music Unmixing/Separation</h1></section>
				<section data-background-transition="none" data-background-image="assets/stones_noguitar.jpg"><h1>Music Unmixing/Separation</h1></section>
				<section data-background-transition="none" data-background-image="assets/stones_noguitar_novocals.jpg"><h1>Music Unmixing/Separation</h1></section>

				<section>
					<h1>Applications</h1>
					<img width="40%" style="float:right" src="assets/karaoke.jpg" alt="">
					
					<ul>
						<li class="fragment">Automatic Karaoke</li>
						<li class="fragment">Creative Music Production</li>
						<li class="fragment">Active listening</li>
						<li class="fragment">Upmixing (stereo $\Rightarrow$ 5.1)</li>
						<li class="fragment">Music Education</li>
						<li class="fragment">Pre-processing for MIR
							<ul class="fragment">
								<li>automatic music transcription</li>
								<li>lyric and music alignment</li>
								<li>musical instrument detection</li>
								<li>lyric recognition</li>
								<li>vocal activity detection</li>
								<li>fundamental frequency estimation</li>
							</ul>
						</li>
					</ul>
				</section>
				
				<section>
					<h1>Task definition</h1>
					
					
					<p><img width="700px" src="https://faroit.com/defense-slides/assets/images/mixture_model.svg" alt=""></p>
					
					<ul class="fragment">
						<li>$s$ be an instrument recording</li>
						<li>$x$ is the observed mixture signal</li>
					</ul>
					
					<ul class="fragment" >
						<li>Estimate the original signals $s$ from the measurement $x$</li>
						<li>$k$ is known</li>
						<li>we only have a single observation: 	<b>under-determined</b></li>
					</ul> 
					<p></p>
					<h3 class="fragment">Inverse Problem</h3>
					
					
				</section>

				<section class="fig-container" data-background-color="#000000" data-file="https://www.youtube.com/embed/IxLnoy-GzqI">
				</section>

				<section>
					<h1>Classical approach: an inverse problem</h1>
					<img class="fragment" width="100%" style="float:center" src="assets/sources_mixing_separation.svg" alt="">
				</section>
				
				<section>
					<section>
						<h1>Music separation as a machine learning problem</h1>
						<h2> Generative or discriminative </h2>
						<img width="100%" src="assets/generative-discriminative.svg" alt="">
					</section>
				
					<section>
						<h1>Music separation as a machine learning problem</h1>
						<h2>Classification ...</h2>
						<img width="66%" src="assets/binarymasking.svg" alt="">
						<h4>Binary Masking</h4>
						<h2> ... or regression ?</h2>
						<img width="50%" src="assets/softmask.svg" alt="">
						<h4>Softmask</h4>
						<img width="50%" src="assets/direct.svg" alt="">
						<h4>Magnitude Spectrogram</h4>
					</section>
				
					<section>
						<h1>Music separation as a machine learning problem</h1>
						<h2>Supervised ... </h2>
						<img style="float:right"
							src="https://docs.google.com/drawings/d/e/2PACX-1vRXc_l4uNUTSOoAif8r4O-AKfAVBBMUSPVG_VMu79LjcZLb4xKLgFTVoSqVodvGetEvdeakfb4Nul-3/pub?w=400"
							alt="">
						<ul>
							<li>Single I/O: modeling sources independently</li>
							<li>Multiple I/O: modeling sources jointly</li>
							<li>Siamese networks, Chimera Networks</li>
						</ul>
				
						<h2>... or unsupervised ? <font color="red">(open direction)</font>
						</h2>
				
					</section>
				
					<section>
						<h1>Music separation as a machine learning problem</h1>
						<h2>Modeling fixed-sized spectrograms ... ?</h2>
						<ul>
							<li>Separating chunks: straightforward reuse of image models</li>
							<li>Batching over chunks</li>
							<li>Fully connected, etc</li>
						</ul>
						<h2>... or learning dynamic models ?</h2>
						<ul>
							<li>Very long-term dependencies !</li>
							<li>LSTM, CNN, etc</li>
						</ul>
					</section>
				</section>

				<section data-background-transition="none" data-state="no-title-footer"
					data-background-image="assets/demotrack/mix.jpg">
					<h1 style="margin-top:5%; color:white">Mixture spectrogram</h1>
					<audio controls src="assets/demotrack/mixture.m4a"></audio>
				</section>
				<section data-background-transition="none">
					<section data-background-transition="none" data-state="no-title-footer"
						data-background-image="assets/demotrack/vocals.jpg">
						<h1 style="margin-top:5%; color:white">Vocals spectrogram</h1>
						<audio controls src="assets/demotrack/vocals.m4a"></audio>
					</section>
					<section data-background-transition="none" data-state="no-title-footer"
						data-background-image="assets/demotrack/drums.jpg">
						<h1 style="margin-top:5%; color:white">Drums spectrogram</h1>
						<audio controls src="assets/demotrack/drums.m4a"></audio>
					</section>
					<section data-background-transition="none" data-state="no-title-footer"
						data-background-image="assets/demotrack/bass.jpg">
						<h1 style="margin-top:5%; color:white">Bass spectrogram</h1>
						<audio controls src="assets/demotrack/bass.m4a"></audio>
					</section>
				</section>

				<section>
					<h1>Basic Separation Architectures</h1>
					<img src="assets/encoder_decoder.png" alt="">

					<ul>
						<li>Encoder here: representation</li>
					</ul>
				</section>

				<section>
					<h1>Why is it a challenging problem for deep learning</h1>
				
					<ul>
						<li>Music has a lot variations</li>
						<li>Music has long dependencies and have different length</li>
						<li>Evaluation metrics (perceptual) not differentiable</li>
						<li>Very little available data</li>
					</ul>
				
					<img width="100%" src="assets/richter.gif" alt="">
					<div class="caption">Image used courtesy of Jan Van Balen.</div>
				</section>


				<section>
					<h1>Basic recipe</h1>
				
					<p>we need...</p>
					
					<ul>
						<li class="fragment">Effective representation (STFT?)</li>
						<li class="fragment">Model predicts a mask: ‚û° $mask \cdot mixture = estimate$</li>
						<li class="fragment">Model should output audio at the same shape as the input</li>
						<li class="fragment">Generalize to different instruments</li>
						<li class="fragment">Utilize stereo information</li>
						<li class="fragment">Audio signal reconstruction: inverse STFT
							<ul>
								<li>Magnitude: the masked mixed audio</li>
								<li>Phase: the phase of the mixed audio</li>
							</ul>
						</li>
					</ul>

				</section>
				
				<section
					data-background-image="https://www.asimovinstitute.org/wp-content/uploads/2019/04/NeuralNetworkZoo20042019.png">
				</section>

				<section>
					<h1>Basic model: Separating auto-encoder</h1>
					
					<ul>
						<li>Encoder: dense layers the input into a latent vector</li>
						<li>Decoder: dense layers reconstruct the input from the latent vector</li>
						<li>Complex structures can be learned with deeper layers</li>
						<li>Weight matrices are typically symmetric between the encoder
						and decoder</li>
						<li>Trained to minimize the reconstruction error between input and output</li>
					</ul>
					<img src="assets/autoencoder.png" alt="">
				</section>

				<section>
					<h1>Basic Encoder: Fixed time frequency representations</h1>
					<img width="100%" style="margin-top:5%; float:left" src="assets/STFT.svg" alt="">
					
				</section>
			
				<section>
					<h1>A Baseline System</h1>
					<h2>Pre-Processing</h2>
					<ul>
						<li><b>Time-Frequency Transform (STFT/ISTFT = Encoder/Decoder)</b></li>
						<li><b>Normalization:</b> Gain Variation</li>
						<li><b>Standardization:</b> Scale Frequency Bands</li>
					</ul>

				</section>
				
				<section>
					<h1>Training Dataset: MUSDB18</h1>
					<ul>
						<li>100 train / 50 test full tracks</li>
						<li>Mastered with pro. digital audio workstations</li>
						<li>compressed STEMS (<code>MUSDB18</code>) and uncompressed WAV <code>MUSDB18-HQ</code>
						<li>Parser and Evaluation tools in <i class="fab fa-python"></i></li>
						<li><a ref=https://sigsep.github.io/datasets/musdb.html>https://sigsep.github.io/datasets/musdb.html </a> </li>
					</ul> <img width="320px" style="float: right" src="assets/ni_logo.png" alt="">
					<img width="60%" src="assets/hero.svg" alt="">
				</section>

				<section>
					<h1>A Baseline System</h1>
					<h2>Sampling for Training</h2>
					<img width="400px" style="float:right" src="assets/sampling.gif" alt="">
					<ul>
						<li>Slicing Temporal Context
							<ul>
								<li>Full tracks too large (vanishing gradient)</li>
								<li>Context usually 1-10 seconds</li>
							</ul>
						</li>
						<li>Batch from different Tracks</li>
						<li>Data Augmentation
							<ul>
								<li>Image Augmenations doesn't work</li>
								<li>Apply Random Gains</li>
							</ul>
						</li>
					</ul>
				</section>
				
				<section>
					<h1>Loss functions</h1>

					<ul>
						<li>Spectrogram losses
							<ul>
								<li>squared loss $\left|X-Y\right|^2$</li>
								<li>absolute loss $\left|X-Y\right|$</li>
								<li>Kullback Leibler loss</li>
								<li>Itakura Saito loss</li>
								<li>Cauchy, alpha divergence, ...<p>
								</li>
							</ul>
						</li>
						
						<li>Waveform losses</li>
						<li>Combination losses</li>

						<li>For now: L1 or L2</li>
					</ul>
					<p></p>
					<a href="https://www.sisec17.audiolabs-erlangen.de/#/listen/34/UHL1">Demo: https://www.sisec17.audiolabs-erlangen.de/#/listen/34/UHL1</a>

				</section>
				
				<section>
					<h1>Recurrent Model, learning longer term structure</h1>
					<img height="300px" style="float:right" src="assets/flow.svg" alt="">
				
					<h2>Spectrogram Architectures </h2>
					<ul>
						<li><b>Bidirectional LSTM <b style="color:red">[Huang 2014, Uhlich 2015, Takashi 2018]</b></b></li>
						<li>Sequence2Sequence:</li>
						<li><b>Input (mix):</b> (sample, frames, frequency)</li>
						<li><b>Output (target):</b> (sample, frames, frequency)</li>
					</ul>
					<p></p>
					<ul>
						<li><a href="https://www.sisec17.audiolabs-erlangen.de/#/listen/34/UHL2">Demo: https://www.sisec17.audiolabs-erlangen.de/#/listen/34/UHL2</a></li>
					</ul>
					
				</section>

				<section>
					<h1>Open-Unmix Baseline</h1>
					<img style="margin-top:2%; float:left" width="160%" src="assets/open-unmix-master_branch1.svg" alt="">
				</section>
				<section>
					<h1>Open-Unmix Baseline</h1>
					<img style="margin-top:2%; float:left" width="160%" src="assets/open-unmix-master_branch2.svg" alt="">
				</section>
				<section>
					<h1>Open-Unmix Baseline</h1>
					<img style="margin-top:2%; float:left" width="160%" src="assets/open-unmix-master_branch3.svg" alt="">
				</section>

				<section>
					<h1>The pytorch <code>Open-unmix</code> (UMX) model</h1>
					<img style="margin-top:2%; float:left" width="160%" src="assets/UMX1.svg" alt="">
				</section>
				<section>
					<h1>The pytorch <code>Open-unmix</code> (UMX) model</h1>
					<img style="margin-top:2%; float:left" width="160%" src="assets/UMX2.svg" alt="">
					<ul>
						<li>MIT-licensed Pytorch implementation</li>
						<li><a href=https://github.com/sigsep/open-unmix-pytorch> <i
								class="fab fa-github"></i>/sigsep/open-unmix-pytorch</a> 6.3 dB vocals SDR!</li>
					</ul>
					<h6 style="margin-top:4%;color:gray">
						F. St√∂ter et al, "Open-Unmix - A reference implementation for audio source separation", JOSS 2019.
					</h6>

				</section>

				<section>
					<h1>Evaluation</h1>
					
					<img src="https://sigsep.github.io/eusipco2019_tutorial/assets/boxplot_full.svg" alt="">
				</section>

				<section>
					<h1>Discussion</h1>
				</section>

				<section class="fig-container" data-background-color="#000000" data-file="https://www.youtube.com/embed/7ezi7EFtZrU">
					<!-- demo video 1 -->
				</section>
				
				<section class="fig-container" data-background-color="#000000" data-file="https://www.youtube.com/embed/MMLsDLF1DfI">
					<!-- demo video 2 -->
				</section>

				<section>
					<h1>Convolutional Encoder/Decoder?</h1>
				
					<div align="center">
						<img src="assets/encoder_decoder.png" alt="">
					</div>
				</section>

				<section>
					<h1>Convolutional Neural Networks</h1>
				
					<div align="center">
						<img width="700px" src="assets/classification.png" alt="">
					</div>
					<div class="caption">Image: <i>Deep Learning with PyTorch: Build, Train, and Tune Neural Networks Using Python Tools
					Book by Eli Stevens and Thomas Viehmann</i></div>

				</section>


				<section>
					<h1>Typical ConvNet for Audio</h1>
				
					<ul>
						<li>Output: softmax or sigmoid layer</li>
						<li>Loss function is the e.g. cross-entropy between the output and the label vector</li>
						<li>Appropriate in audio-to-tag or audio-to-score (MIDI, beat, chord) task</li>
					</ul>
					<img style="float: right" width="800px" src="https://raw.githubusercontent.com/gojibjib/jibjib-model/master/assets/Typical_cnn_spectrogram.png" alt="">
					<div style="float: right" class="caption">Image: <i>https://github.com/gojibjib/jibjib-model</i></div>

				</section>

				<section>
					<h1>Segmentation (=Separation?)</h1>
				
					<div align="center">
						<img width="700px" src="assets/cat_segment.png" alt="">
					</div>
										<div class="caption">Image: <i>Deep Learning with PyTorch: Build, Train, and Tune Neural Networks Using Python Tools
												Book by Eli Stevens and Thomas Viehmann</i></div>

				</section>
								
				<section>
					<h1>ConvNet for Audio-to-audio</h1>
					<ul>
						<li>Source separation, style transfer, denoising, enhancement</li>
						<li>Inverse architecture converts the feature space back to the audio representation
						</ul>
						<p></p>
						<h3>How to design upsampling network</h3>
						
						<img style="float: right" width="120px" src="assets/encoder-decoder.png" alt="">
						<ul>
							<li>Padding + Convolution (simple, limited receptive field)</li>
							<li>How can we improve the receptive field and get 1:1 ratio of
								input/output?</li>
							</ul>
							
					</section>

				<section>
					<h1>Upsampling</h1>
				
					<ul>
						<li>Unpooling (parameter free)</li>
						<img src="assets/unpooling.png" alt="">
				
					</ul>
				
					<ul>
						<li>Deconvolution (=upsampled convolution)</li>
						<img src="assets/deconvolution.png" alt="">
					</ul>
					<div class="caption">Image: Learning deconvolution network for semantic segmentation, H. Noh, S. Hong, B. Han,</div>

				</section>

				<section>
					<h1>upsampled convolutions?</h1>
					<h1>‚úçÔ∏è</h1>
				</section>

				<section>
					<h1>Fully convolutional network (FCN)</h1>
					<p></p>
					<p></p>
					<p></p>
					<img src="https://miro.medium.com/max/3486/0*jIBAjzSynvcV-lvO.png" width="900em" alt="">
					<ul>
						<li>Misses fine structure needed for separation</li>
					</ul>
					<div class="caption">Image: Learning Deconvolution Network for Semantic Segmentation
					Hyeonwoo Noh, Seunghoon Hong, Bohyung Han</div>

				</section>


				<section>
					<h1>U-net</h1>
					<ul>
						<li>First introduced for medical image segmentation</li>
					</ul>
					<img src="assets/u-net-image.png" width="700em" alt="">
					<div class="caption">Image: U-Net: Convolutional Networks for Biomedical Image Segmentation
						Olaf Ronneberger, Philipp Fischer, Thomas Brox</div>

				</section>

				<section>
					<h1>Main contribution: Skip connection</h1>
					
					<ul>
						<li>known from ResNet? Here applied differently</li>	
						<li>Here: skip connections short-circuit inputs along the downsampling path</li>
						<li>Layers concatenate (or sum):
							<ul>
								<li>1. input from upsampled results (large receptive field)</li>
								<li>2. output of the earlier fine detail</li>
							</ul>
						</li>
					</ul>
					<img style="float: right" width="400px" src="https://miro.medium.com/max/960/1*dJRA7-IjccrkCWw86V-oUg.gif" alt="">

				</section>
			
				<section>
					<h1>Spectrogram U-net</h1>
					<div align="center"><img src="assets/u-net-block.png" width="600em" alt=""></div>
					
				<div class="caption">Image: Singing Voice Separation with Deep U-Net Convolutional Networks, Jansson et. al.</div>

				</section>

				<section>
					<h1>Details</h1>

					<ul>
						<li>Waveform input resampled to 8kHz, spectrogram is normalized to the range [0,1]</li>
						<li>No pooling</li>
						<li>L1 norm as loss function</li>
						<li>5x5 Filter size of stride 2 in both conv and upsampled conv layers</li>
					</ul>
				</section>	
				
				<section>
					<h1>Open-Unmix vs. Spleeter vs. ConvtasNet vs. Demucs. </h1>

									<div style="margin-left: 8em">
					<ul>
						<li></li>
					</ul>
					<iframe scrolling="no" allow="autoplay"
						src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/playlists/934126789&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=false&amp;show_user=false&amp;show_reposts=false&amp;show_teaser=true"
						width="500" height="450" frameborder="no"></iframe>
									</div>

				</section>

				<section>
					<h1>Are upsampled convolution safe for audio?</h1>
					<h1>‚úçÔ∏è</h1>
				</section>

				<section>
					<h1>Upsampled Convolution</h1>
					<div style="margin-left: 6em; margin-top: 6em;">
						<iframe width="690px" height="300px" data-src="assets/transposedconv/deconv1d.html" data-preload></iframe>
					</div>
					<small style="position: absolute; right: 0px"><a href="https://distill.pub/2016/deconv-checkerboard/">https://distill.pub/2016/deconv-checkerboard/</a></small>
				</section>

				<section>
					<h1>2D upsampled Convolution</h1>

				<div style="margin-left: 6em; margin-top: 6em;">
					<iframe width="690px" height="400px" data-src="assets/transposedconv/deconv2d.html" data-preload></iframe>
				</div>
				<small style="position: absolute; right: 0px"><a
						href="https://distill.pub/2016/deconv-checkerboard/">https://distill.pub/2016/deconv-checkerboard/</a></small>
	
				</section>

				<section>
					<h1>Issues with audio upsampling</h1>

					<img style="float: right" width="320px" src="assets/upsampling_jordi.png" alt="">
					<ul>
						<li>Weight initialization</li>
						<li>Upsampled convolution (except stride == kernel size)</li>
						<li>Training</li>
					</ul>
					<p></p>
					<div class="caption">Upsampling artifacts in neural audio synthesis
					Jordi Pons, Santiago Pascual, Giulio Cengarle, Joan Serr√†</div>

				</section>

				
				<section>
					<h1>Music separation vs. Speech Separation</h1>
					<h2>...in typical applications</h2>
				
					<table>
						<thead>
							<tr>
								<th><strong></strong></th>
								<th><strong>üé§ Speech</strong></th>
								<th><strong>üéπ Music</strong></th>
							</tr>
						</thead>
						<tbody>
							<tr class="fragment">
								<td>sample rate</td>
								<td class="fragment">4000-16000 kHz</td>
								<td class="fragment">>=44100 kHz</td>
							</tr>
							<tr class="fragment">
								<td>typical signal length</td>
								<td class="fragment">seconds</td>
								<td class="fragment">minutes</td>
							</tr>
							<tr class="fragment">
								<td># sources</td>
								<td class="fragment">1 to 4</td>
								<td class="fragment">1 to 20</td>
							</tr>
							<tr class="fragment">
								<td># channels</td>
								<td class="fragment">1</td>
								<td class="fragment">2</td>
							</tr>
							<tr class="fragment">
								<td>permutation</td>
								<td class="fragment">variant</td>
								<td class="fragment">fixed</td>
							</tr>
							<tr class="fragment">
								<td>Mixing model</td>
								<td class="fragment">linear/convolutive</td>
								<td class="fragment">non-linear</td>
							</tr>
							<tr class="fragment">
								<td>Degradations</td>
								<td class="fragment">noise, reverberation</td>
								<td class="fragment">distortion, compression</td>
							</tr>

						</tbody>
					</table>
				</section>
	
				<section>
					<h1>Issues with magnitude spectrograms</h1>
					<ul>
						<li>STFT depends on many parameters: window size, hop size, window type</li>
						<li>Phase information is not used</li>
						<li>Using the mixture phase for reconstruction may cause artifacts</li>
					</ul>
					<p></p>
					<h5 class="fragment" >Solution 1: complex spectrograms</h5>
					<img class="fragment" style="float: right" width="350px" src="assets/convtasnet-kernels.png" alt="">
					<h5 class="fragment" >Solution 2: learnable transform - Conv-Tasnet (SOTA for speech separation)</h5>
					
				</section>

				<section>
				
					<h1>Demo: Spectrogram vs time domain</h1>
					<ul>
						<li>Both methods have almost the same SDR</li>
					</ul>
					<iframe src="https://share.unmix.app/KoyqGF6lrcqBYkYZBYze" width="900px" height="500px" frameborder="0"></iframe>
				
				</section>

				<section>
					<h1>Trends</h1>

					<ul>
						<li class="fragment">From ‚ÄúSupervised‚Äù to ‚ÄúUniversal‚Äù Separation</li>
						<li class="fragment">Dealing with ‚ÄúImperfect‚Äù Training Data</li>
						<li class="fragment">‚ÄúImperfect‚Äù = unknown mixtures of sources</li>
						<li class="fragment">Perceptual Loss Functions</li>
						<li class="fragment">Multi-Task Training</li>
						<li class="fragment">From Research to Deployment</li>
					</ul>
				</section>

				<section>
					<h1>Is the problem solved?</h1>
					
					<ul>
						<li>‚ÄúA system that achieves human auditory analysis performance in all listening situation‚Äù
						(Wang)</li>
						<li>Thanks to deep learning a lot of progress has been made‚ÄìBut from the contest results we know that we are not
							there yet</li>
					</ul>
				</section>
				<section>
					<h1>Music Demixing Challenge</h1>
					<!-- mention time frequency challenges -->
					<img src="assets/musdemix.png" alt="">
					<a href="https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021">https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021</a>

				</section>

				<section>
					<h1>Ressources</h1>

					<ul>
						<li><a href="https://sigsep.github.io">sigsep.github.io</a> Dataset and Baseline implementations</li>
						<li><a href="https://source-separation.github.io">source-separation.github.io</a> Separation Tutorial:</li>
					</ul>
					<img src="https://sigsep.github.io/hero.png" alt="">
				</section>

			</div>
			<div class='footer'>
				<div style="color:black; font-size: 20px;" id="middlebox">DLA: Music Source Separation</div>
			</div>
		</div>

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: false,
				slideNumber: true,
				minScale: 0.1,
				maxScale: 8,
				transition: 'none', //

				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/math-katex/math-katex.js', async: true },
					{ src: 'plugin/reveald3/reveald3.js' },
					{ src: 'plugin/highlight/highlight.js', async: true }
				]
			});
		</script>
	</body>
</html>
